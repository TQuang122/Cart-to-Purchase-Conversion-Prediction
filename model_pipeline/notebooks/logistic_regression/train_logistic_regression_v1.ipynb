{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Logistic Regression Baseline Training\n",
    "## Customer Purchase Propensity Prediction\n",
    "\n",
    "This notebook trains a Logistic Regression model following the plan:\n",
    "1. Load data from Feast Feature Store (parquet file)\n",
    "2. Preprocessing: StandardScaler for numerical, OneHotEncoder for categorical\n",
    "3. Train/Val/Test split: 64%/16%/20%\n",
    "4. Regularization tuning on validation set\n",
    "5. Evaluate with Accuracy, Precision, Recall, F1, AUC-ROC\n",
    "6. Save metrics to JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    ")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load-data-header",
   "metadata": {},
   "source": [
    "## 1. Load Data from Feast Feature Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"LOGISTIC REGRESSION BASELINE TRAINING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\n[1/6] Loading data from Feast Feature Store...\")\n",
    "\n",
    "# Define path - adjust based on notebook location\n",
    "parquet_path = Path(\"../../../data_pipeline/propensity_feature_store/propensity_features/feature_repo/data/processed_purchase_propensity_data_v1.parquet\")\n",
    "parquet_path = parquet_path.resolve()\n",
    "\n",
    "print(f\"Loading from: {parquet_path}\")\n",
    "df = pd.read_parquet(parquet_path)\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Columns: {df.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "explore-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the data\n",
    "print(\"First 5 rows:\")\n",
    "display(df.head())\n",
    "\n",
    "print(f\"\\nData types:\")\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "features-header",
   "metadata": {},
   "source": [
    "## 2. Define Features and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "define-features",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n[2/6] Preparing features and preprocessing pipeline...\")\n",
    "\n",
    "NUMERICAL_FEATURES = [\"price\", \"activity_count\", \"event_weekday\"]\n",
    "CATEGORICAL_FEATURES = [\"brand\", \"category_code_level1\", \"category_code_level2\"]\n",
    "TARGET = \"is_purchased\"\n",
    "ALL_FEATURES = NUMERICAL_FEATURES + CATEGORICAL_FEATURES\n",
    "\n",
    "print(f\"Numerical features: {NUMERICAL_FEATURES}\")\n",
    "print(f\"Categorical features: {CATEGORICAL_FEATURES}\")\n",
    "print(f\"Target: {TARGET}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prepare-xy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare X and y\n",
    "X = df[ALL_FEATURES].copy()\n",
    "y = df[TARGET].copy()\n",
    "\n",
    "# Convert categorical columns to string type\n",
    "for col in CATEGORICAL_FEATURES:\n",
    "    X[col] = X[col].astype(str)\n",
    "\n",
    "print(f\"\\nTarget distribution:\")\n",
    "print(f\"  Class 0 (Not Purchased): {(y == 0).sum():,} ({(y == 0).mean() * 100:.2f}%)\")\n",
    "print(f\"  Class 1 (Purchased):     {(y == 1).sum():,} ({(y == 1).mean() * 100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "split-header",
   "metadata": {},
   "source": [
    "## 3. Train/Validation/Test Split (64%/16%/20%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "split-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n[3/6] Splitting data (64%/16%/20%)...\")\n",
    "\n",
    "# First split: 80% train+val, 20% test\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Second split: 80% train, 20% val (of the 80% = 64% and 16% of total)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_val, y_train_val, test_size=0.2, random_state=42, stratify=y_train_val\n",
    ")\n",
    "\n",
    "print(f\"Training set:   {X_train.shape[0]:,} samples ({X_train.shape[0] / len(X) * 100:.1f}%)\")\n",
    "print(f\"Validation set: {X_val.shape[0]:,} samples ({X_val.shape[0] / len(X) * 100:.1f}%)\")\n",
    "print(f\"Test set:       {X_test.shape[0]:,} samples ({X_test.shape[0] / len(X) * 100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-preprocessor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create preprocessor\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", StandardScaler(), NUMERICAL_FEATURES),\n",
    "        (\n",
    "            \"cat\",\n",
    "            OneHotEncoder(\n",
    "                handle_unknown=\"ignore\", sparse_output=False, max_categories=100\n",
    "            ),\n",
    "            CATEGORICAL_FEATURES,\n",
    "        ),\n",
    "    ],\n",
    "    remainder=\"drop\",\n",
    ")\n",
    "\n",
    "print(\"Preprocessor created!\")\n",
    "print(preprocessor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tuning-header",
   "metadata": {},
   "source": [
    "## 4. Regularization Tuning on Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tuning",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n[4/6] Tuning regularization parameter C...\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "C_VALUES = [0.001, 0.01, 0.1, 1, 10, 100]\n",
    "tuning_results = []\n",
    "\n",
    "for C in C_VALUES:\n",
    "    start_time = time.time()\n",
    "    print(f\"\\nTraining with C={C}...\", end=\" \", flush=True)\n",
    "\n",
    "    # Create pipeline\n",
    "    pipeline = Pipeline(\n",
    "        [\n",
    "            (\"preprocessor\", preprocessor),\n",
    "            (\n",
    "                \"classifier\",\n",
    "                LogisticRegression(\n",
    "                    C=C,\n",
    "                    solver=\"lbfgs\",\n",
    "                    max_iter=1000,\n",
    "                    class_weight=\"balanced\",\n",
    "                    random_state=42,\n",
    "                    n_jobs=-1,\n",
    "                ),\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Fit on training data\n",
    "    pipeline.fit(X_train, y_train)\n",
    "\n",
    "    # Predict on validation set\n",
    "    y_val_pred = pipeline.predict(X_val)\n",
    "    y_val_proba = pipeline.predict_proba(X_val)[:, 1]\n",
    "\n",
    "    # Calculate metrics\n",
    "    val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "    val_f1 = f1_score(y_val, y_val_pred, average=\"macro\")\n",
    "    val_auc = roc_auc_score(y_val, y_val_proba)\n",
    "\n",
    "    elapsed = time.time() - start_time\n",
    "\n",
    "    result = {\n",
    "        \"C\": C,\n",
    "        \"accuracy\": val_accuracy,\n",
    "        \"f1_macro\": val_f1,\n",
    "        \"auc_roc\": val_auc,\n",
    "        \"pipeline\": pipeline,\n",
    "    }\n",
    "    tuning_results.append(result)\n",
    "\n",
    "    print(f\"Done ({elapsed:.1f}s)\")\n",
    "    print(f\"  Accuracy: {val_accuracy:.4f} | F1: {val_f1:.4f} | AUC-ROC: {val_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "select-best",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select best model\n",
    "best_result = max(tuning_results, key=lambda x: x[\"auc_roc\"])\n",
    "best_C = best_result[\"C\"]\n",
    "best_pipeline = best_result[\"pipeline\"]\n",
    "\n",
    "print(\"\\n\" + \"-\" * 50)\n",
    "print(f\"Best C: {best_C} (AUC-ROC: {best_result['auc_roc']:.4f})\")\n",
    "\n",
    "# Summary table\n",
    "print(\"\\nTuning Summary:\")\n",
    "tuning_df = pd.DataFrame([\n",
    "    {\"C\": r[\"C\"], \"Accuracy\": r[\"accuracy\"], \"F1 Macro\": r[\"f1_macro\"], \"AUC-ROC\": r[\"auc_roc\"]}\n",
    "    for r in tuning_results\n",
    "])\n",
    "display(tuning_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "final-training-header",
   "metadata": {},
   "source": [
    "## 5. Final Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final-training",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n[5/6] Final training on train+validation...\")\n",
    "\n",
    "# Combine train and validation\n",
    "X_train_final = pd.concat([X_train, X_val], axis=0)\n",
    "y_train_final = pd.concat([y_train, y_val], axis=0)\n",
    "\n",
    "print(f\"Final training set: {len(X_train_final):,} samples\")\n",
    "\n",
    "# Create final pipeline\n",
    "final_preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", StandardScaler(), NUMERICAL_FEATURES),\n",
    "        (\n",
    "            \"cat\",\n",
    "            OneHotEncoder(\n",
    "                handle_unknown=\"ignore\", sparse_output=False, max_categories=100\n",
    "            ),\n",
    "            CATEGORICAL_FEATURES,\n",
    "        ),\n",
    "    ],\n",
    "    remainder=\"drop\",\n",
    ")\n",
    "\n",
    "final_pipeline = Pipeline(\n",
    "    [\n",
    "        (\"preprocessor\", final_preprocessor),\n",
    "        (\n",
    "            \"classifier\",\n",
    "            LogisticRegression(\n",
    "                C=best_C,\n",
    "                solver=\"lbfgs\",\n",
    "                max_iter=1000,\n",
    "                class_weight=\"balanced\",\n",
    "                random_state=42,\n",
    "                n_jobs=-1,\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "start_time = time.time()\n",
    "final_pipeline.fit(X_train_final, y_train_final)\n",
    "train_time = time.time() - start_time\n",
    "print(f\"Training complete ({train_time:.1f}s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "evaluate-test",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "print(\"\\nEvaluating on test set...\")\n",
    "y_test_pred = final_pipeline.predict(X_test)\n",
    "y_test_proba = final_pipeline.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculate all metrics\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "test_precision_macro = precision_score(y_test, y_test_pred, average=\"macro\")\n",
    "test_recall_macro = recall_score(y_test, y_test_pred, average=\"macro\")\n",
    "test_f1_macro = f1_score(y_test, y_test_pred, average=\"macro\")\n",
    "test_auc_roc = roc_auc_score(y_test, y_test_proba)\n",
    "\n",
    "# Per-class metrics\n",
    "test_precision_per_class = precision_score(y_test, y_test_pred, average=None)\n",
    "test_recall_per_class = recall_score(y_test, y_test_pred, average=None)\n",
    "test_f1_per_class = f1_score(y_test, y_test_pred, average=None)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"TEST SET RESULTS\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Accuracy:  {test_accuracy:.4f}\")\n",
    "print(f\"Precision: {test_precision_macro:.4f} (macro)\")\n",
    "print(f\"Recall:    {test_recall_macro:.4f} (macro)\")\n",
    "print(f\"F1-Score:  {test_f1_macro:.4f} (macro)\")\n",
    "print(f\"AUC-ROC:   {test_auc_roc:.4f}\")\n",
    "\n",
    "print(\"\\nPer-Class Metrics:\")\n",
    "print(f\"  Class 0 (Not Purchased):\")\n",
    "print(f\"    Precision: {test_precision_per_class[0]:.4f}\")\n",
    "print(f\"    Recall:    {test_recall_per_class[0]:.4f}\")\n",
    "print(f\"    F1-Score:  {test_f1_per_class[0]:.4f}\")\n",
    "print(f\"  Class 1 (Purchased):\")\n",
    "print(f\"    Precision: {test_precision_per_class[1]:.4f}\")\n",
    "print(f\"    Recall:    {test_recall_per_class[1]:.4f}\")\n",
    "print(f\"    F1-Score:  {test_f1_per_class[1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confusion-matrix",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_test_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(f\"  [[TN={cm[0, 0]:,}  FP={cm[0, 1]:,}]\")\n",
    "print(f\"   [FN={cm[1, 0]:,}  TP={cm[1, 1]:,}]]\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_test_pred, target_names=[\"Not Purchased\", \"Purchased\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "validation-metrics",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get validation metrics for the best model\n",
    "y_val_pred_best = best_pipeline.predict(X_val)\n",
    "y_val_proba_best = best_pipeline.predict_proba(X_val)[:, 1]\n",
    "\n",
    "val_accuracy = accuracy_score(y_val, y_val_pred_best)\n",
    "val_precision_macro = precision_score(y_val, y_val_pred_best, average=\"macro\")\n",
    "val_recall_macro = recall_score(y_val, y_val_pred_best, average=\"macro\")\n",
    "val_f1_macro = f1_score(y_val, y_val_pred_best, average=\"macro\")\n",
    "val_auc_roc = roc_auc_score(y_val, y_val_proba_best)\n",
    "\n",
    "val_precision_per_class = precision_score(y_val, y_val_pred_best, average=None)\n",
    "val_recall_per_class = recall_score(y_val, y_val_pred_best, average=None)\n",
    "val_f1_per_class = f1_score(y_val, y_val_pred_best, average=None)\n",
    "\n",
    "print(\"Validation Set Metrics (for comparison):\")\n",
    "print(f\"  Accuracy:  {val_accuracy:.4f}\")\n",
    "print(f\"  Precision: {val_precision_macro:.4f} (macro)\")\n",
    "print(f\"  Recall:    {val_recall_macro:.4f} (macro)\")\n",
    "print(f\"  F1-Score:  {val_f1_macro:.4f} (macro)\")\n",
    "print(f\"  AUC-ROC:   {val_auc_roc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "save-header",
   "metadata": {},
   "source": [
    "## 6. Save Metrics to JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save-metrics",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n[6/6] Saving metrics to JSON...\")\n",
    "\n",
    "metrics = {\n",
    "    \"model\": \"LogisticRegression\",\n",
    "    \"timestamp\": datetime.now().isoformat(),\n",
    "    \"hyperparameters\": {\n",
    "        \"best_C\": best_C,\n",
    "        \"solver\": \"lbfgs\",\n",
    "        \"max_iter\": 1000,\n",
    "        \"class_weight\": \"balanced\",\n",
    "    },\n",
    "    \"data_split\": {\n",
    "        \"train_size\": int(len(X_train)),\n",
    "        \"val_size\": int(len(X_val)),\n",
    "        \"test_size\": int(len(X_test)),\n",
    "        \"train_val_size\": int(len(X_train_final)),\n",
    "        \"total_size\": int(len(X)),\n",
    "    },\n",
    "    \"features\": {\n",
    "        \"numerical\": NUMERICAL_FEATURES,\n",
    "        \"categorical\": CATEGORICAL_FEATURES,\n",
    "        \"preprocessing\": {\n",
    "            \"numerical\": \"StandardScaler\",\n",
    "            \"categorical\": \"OneHotEncoder (max_categories=100)\",\n",
    "        },\n",
    "    },\n",
    "    \"regularization_tuning\": [\n",
    "        {\n",
    "            \"C\": r[\"C\"],\n",
    "            \"val_accuracy\": round(r[\"accuracy\"], 4),\n",
    "            \"val_f1_macro\": round(r[\"f1_macro\"], 4),\n",
    "            \"val_auc_roc\": round(r[\"auc_roc\"], 4),\n",
    "        }\n",
    "        for r in tuning_results\n",
    "    ],\n",
    "    \"validation_metrics\": {\n",
    "        \"accuracy\": round(val_accuracy, 4),\n",
    "        \"precision\": {\n",
    "            \"macro\": round(val_precision_macro, 4),\n",
    "            \"class_0\": round(float(val_precision_per_class[0]), 4),\n",
    "            \"class_1\": round(float(val_precision_per_class[1]), 4),\n",
    "        },\n",
    "        \"recall\": {\n",
    "            \"macro\": round(val_recall_macro, 4),\n",
    "            \"class_0\": round(float(val_recall_per_class[0]), 4),\n",
    "            \"class_1\": round(float(val_recall_per_class[1]), 4),\n",
    "        },\n",
    "        \"f1\": {\n",
    "            \"macro\": round(val_f1_macro, 4),\n",
    "            \"class_0\": round(float(val_f1_per_class[0]), 4),\n",
    "            \"class_1\": round(float(val_f1_per_class[1]), 4),\n",
    "        },\n",
    "        \"auc_roc\": round(val_auc_roc, 4),\n",
    "    },\n",
    "    \"test_metrics\": {\n",
    "        \"accuracy\": round(test_accuracy, 4),\n",
    "        \"precision\": {\n",
    "            \"macro\": round(test_precision_macro, 4),\n",
    "            \"class_0\": round(float(test_precision_per_class[0]), 4),\n",
    "            \"class_1\": round(float(test_precision_per_class[1]), 4),\n",
    "        },\n",
    "        \"recall\": {\n",
    "            \"macro\": round(test_recall_macro, 4),\n",
    "            \"class_0\": round(float(test_recall_per_class[0]), 4),\n",
    "            \"class_1\": round(float(test_recall_per_class[1]), 4),\n",
    "        },\n",
    "        \"f1\": {\n",
    "            \"macro\": round(test_f1_macro, 4),\n",
    "            \"class_0\": round(float(test_f1_per_class[0]), 4),\n",
    "            \"class_1\": round(float(test_f1_per_class[1]), 4),\n",
    "        },\n",
    "        \"auc_roc\": round(test_auc_roc, 4),\n",
    "    },\n",
    "    \"confusion_matrix\": {\n",
    "        \"true_negative\": int(cm[0, 0]),\n",
    "        \"false_positive\": int(cm[0, 1]),\n",
    "        \"false_negative\": int(cm[1, 0]),\n",
    "        \"true_positive\": int(cm[1, 1]),\n",
    "    },\n",
    "}\n",
    "\n",
    "metrics_path = Path(\"../../metrics/logistic_regression_metrics.json\")\n",
    "metrics_path = metrics_path.resolve()\n",
    "metrics_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "with open(metrics_path, \"w\") as f:\n",
    "    json.dump(metrics, f, indent=2)\n",
    "\n",
    "print(f\"Metrics saved to: {metrics_path}\")\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TRAINING COMPLETE!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "display-metrics",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display final metrics\n",
    "print(\"\\nFinal Metrics Summary:\")\n",
    "print(json.dumps(metrics, indent=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
